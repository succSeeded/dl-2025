{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/succSeeded/dl-2025/blob/main/hws/week03_convnets/seminar_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE_NtOG6Tpy9"
      },
      "source": [
        "## Task I: small convolution net\n",
        "### First step\n",
        "\n",
        "Let's create a mini-convolutional network with roughly such architecture:\n",
        "* Input layer\n",
        "* 3x3 convolution with 10 filters and _ReLU_ activation\n",
        "* 2x2 pooling (or set previous convolution stride to 3)\n",
        "* Flatten\n",
        "* Dense layer with 100 neurons and _ReLU_ activation\n",
        "* 10% dropout\n",
        "* Output dense layer.\n",
        "\n",
        "\n",
        "__Convolutional layers__ in torch are just like all other layers, but with a specific set of parameters:\n",
        "\n",
        "__`...`__\n",
        "\n",
        "__`model.add_module('conv1', nn.Conv2d(in_channels=3, out_channels=10, kernel_size=3)) # convolution`__\n",
        "\n",
        "__`model.add_module('pool1', nn.MaxPool2d(2)) # max pooling 2x2`__\n",
        "\n",
        "__`...`__\n",
        "\n",
        "\n",
        "Once you're done (and compute_loss no longer raises errors), train it with __Adam__ optimizer with default params (feel free to modify the code above).\n",
        "\n",
        "If everything is right, you should get at least __50%__ validation accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============\n",
        "# some prep work\n",
        "# ==============\n",
        "\n",
        "import time\n",
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from cifar import load_cifar10\n",
        "\n",
        "\n",
        "def compute_loss(X_batch: np.typing.ArrayLike, y_batch: np.typing.ArrayLike) -> torch.types.FloatLikeType:\n",
        "    X_batch = torch.as_tensor(X_batch, dtype=torch.float32, device=device)\n",
        "    y_batch = torch.as_tensor(y_batch, dtype=torch.int64, device=device)\n",
        "    logits = model(X_batch)\n",
        "    return F.cross_entropy(logits, y_batch).mean()\n",
        "\n",
        "\n",
        "# An auxilary function that returns mini-batches for neural network training\n",
        "def iterate_minibatches(X, y, batchsize):\n",
        "    indices = np.random.permutation(np.arange(len(X)))\n",
        "    for start in range(0, len(indices), batchsize):\n",
        "        ix = indices[start: start + batchsize]\n",
        "        yield X[ix], y[ix]\n",
        "\n",
        "\n",
        "# **IMPORTANT** when running in colab, un-comment this\n",
        "!wget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/refs/heads/fall25/week03_convnets/cifar.py\n",
        "\n",
        "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10(\"cifar_data\")\n",
        "\n",
        "class_names = np.array(['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "                        'dog', 'frog', 'horse', 'ship', 'truck'])\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOg3pErJj4jg",
        "outputId": "9f52d5a9-aa48-4d6e-fdb0-56e9fe84503b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-29 19:45:09--  https://raw.githubusercontent.com/yandexdataschool/Practical_DL/refs/heads/fall25/week03_convnets/cifar.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2396 (2.3K) [text/plain]\n",
            "Saving to: ‘cifar.py.2’\n",
            "\n",
            "\rcifar.py.2            0%[                    ]       0  --.-KB/s               \rcifar.py.2          100%[===================>]   2.34K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-09-29 19:45:09 (54.5 MB/s) - ‘cifar.py.2’ saved [2396/2396]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, opt, X_train, y_train, X_val, y_val, num_epochs:int = 100, batch_size:int = 64, stop:int = 7):\n",
        "    \"\"\"\n",
        "    A function for training torch models that I kindly took from the professor (sorry)\n",
        "\n",
        "    Args:\n",
        "        model: torch model to train\n",
        "        opt: optimizer for that model\n",
        "        X_train: training data [n, m], where n is the number of entries and m --- the number of features\n",
        "        y_train: training targets [n,]\n",
        "        X_val: validation data\n",
        "        y_val: validation targets\n",
        "        num_epochs: total amount of full passes over training data (default: 100)\n",
        "        batch_size: number of samples processed in one SGD iteration (default: 64)\n",
        "        stop: number of iterations that loss can decrease for before the training proces stops (default: 7)\n",
        "    \"\"\"\n",
        "    train_loss = []\n",
        "    val_accuracy = []\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # In each epoch, we do a full pass over the training data:\n",
        "        start_time = time.time()\n",
        "        model.train(True) # enable dropout / batch_norm training behavior\n",
        "        for X_batch, y_batch in iterate_minibatches(X_train, y_train, batch_size):\n",
        "            # train on batch\n",
        "            loss = compute_loss(X_batch, y_batch)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "        # And a full pass over the validation data:\n",
        "        model.train(False)     # disable dropout / use averages for batch_norm\n",
        "        with torch.no_grad():  # do not store intermediate activations\n",
        "            for X_batch, y_batch in iterate_minibatches(X_val, y_val, batch_size):\n",
        "                logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "                y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
        "                val_accuracy.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "        mean_val_acc = np.mean(val_accuracy[-len(X_val) // batch_size :])\n",
        "\n",
        "        if best_val_acc < mean_val_acc:\n",
        "            best_val_acc = mean_val_acc\n",
        "            best_epoch = i\n",
        "            pathlib.Path(\"./models/\").mkdir(exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"models/best_model.pt2\")\n",
        "\n",
        "\n",
        "        if epoch - best_epoch > stop:\n",
        "            print(\"Model did not see any loss improvements for %i epochs, aborting...\" % (stop))\n",
        "            model.load_state_dict(torch.load(f\"models/best_model.pt2\", weights_only=True))\n",
        "            model.eval()\n",
        "            break\n",
        "\n",
        "        # Then we print the results for this epoch:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "            np.mean(train_loss[-len(X_train) // batch_size :])))\n",
        "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "            mean_val_acc * 100))\n",
        "\n",
        "    print(f\"Finished training. Best validation accuracy: {best_val_acc*100:.2f} %\")"
      ],
      "metadata": {
        "id": "VaQ3zV-XlX_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, X_test, y_test):\n",
        "    model.train(False) # disable dropout / use averages for batch_norm\n",
        "    test_batch_acc = []\n",
        "    for X_batch, y_batch in iterate_minibatches(X_test, y_test, 500):\n",
        "        logits = model(torch.as_tensor(X_batch, dtype=torch.float32, device=device))\n",
        "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
        "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "    test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "        test_accuracy * 100))"
      ],
      "metadata": {
        "id": "o2uj14sGpzcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us create the model:\n",
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 10, kernel_size=(3,3)),  # [10, 30, 30]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2), # [10, 15, 15]\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(10 * 15 * 15, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(100, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "-cAr5huEVJIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(model, opt, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBMinML4jg8H",
        "outputId": "d6b7b7a3-b581-4049-9a33-3d06f1017eee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100 took 1.418s\n",
            "  training loss (in-iteration): \t1.750762\n",
            "  validation accuracy: \t\t\t46.84 %\n",
            "Epoch 2 of 100 took 1.640s\n",
            "  training loss (in-iteration): \t1.443771\n",
            "  validation accuracy: \t\t\t52.82 %\n",
            "Epoch 3 of 100 took 1.501s\n",
            "  training loss (in-iteration): \t1.346818\n",
            "  validation accuracy: \t\t\t52.65 %\n",
            "Epoch 4 of 100 took 1.395s\n",
            "  training loss (in-iteration): \t1.281974\n",
            "  validation accuracy: \t\t\t55.04 %\n",
            "Epoch 5 of 100 took 1.386s\n",
            "  training loss (in-iteration): \t1.219855\n",
            "  validation accuracy: \t\t\t57.49 %\n",
            "Epoch 6 of 100 took 1.402s\n",
            "  training loss (in-iteration): \t1.178553\n",
            "  validation accuracy: \t\t\t58.09 %\n",
            "Epoch 7 of 100 took 1.387s\n",
            "  training loss (in-iteration): \t1.134880\n",
            "  validation accuracy: \t\t\t58.23 %\n",
            "Epoch 8 of 100 took 1.387s\n",
            "  training loss (in-iteration): \t1.100379\n",
            "  validation accuracy: \t\t\t60.02 %\n",
            "Epoch 9 of 100 took 1.385s\n",
            "  training loss (in-iteration): \t1.063912\n",
            "  validation accuracy: \t\t\t58.82 %\n",
            "Epoch 10 of 100 took 1.544s\n",
            "  training loss (in-iteration): \t1.039553\n",
            "  validation accuracy: \t\t\t60.37 %\n",
            "Epoch 11 of 100 took 1.597s\n",
            "  training loss (in-iteration): \t1.011150\n",
            "  validation accuracy: \t\t\t60.74 %\n",
            "Epoch 12 of 100 took 1.401s\n",
            "  training loss (in-iteration): \t0.985133\n",
            "  validation accuracy: \t\t\t60.95 %\n",
            "Epoch 13 of 100 took 1.394s\n",
            "  training loss (in-iteration): \t0.957662\n",
            "  validation accuracy: \t\t\t61.34 %\n",
            "Epoch 14 of 100 took 1.385s\n",
            "  training loss (in-iteration): \t0.937184\n",
            "  validation accuracy: \t\t\t60.72 %\n",
            "Epoch 15 of 100 took 1.409s\n",
            "  training loss (in-iteration): \t0.914896\n",
            "  validation accuracy: \t\t\t60.56 %\n",
            "Epoch 16 of 100 took 1.391s\n",
            "  training loss (in-iteration): \t0.900764\n",
            "  validation accuracy: \t\t\t62.01 %\n",
            "Epoch 17 of 100 took 1.387s\n",
            "  training loss (in-iteration): \t0.880518\n",
            "  validation accuracy: \t\t\t61.42 %\n",
            "Epoch 18 of 100 took 1.470s\n",
            "  training loss (in-iteration): \t0.859982\n",
            "  validation accuracy: \t\t\t60.85 %\n",
            "Epoch 19 of 100 took 1.699s\n",
            "  training loss (in-iteration): \t0.849732\n",
            "  validation accuracy: \t\t\t60.89 %\n",
            "Model did not see any loss improvements for 7 epochs, aborting...\n",
            "Finished training. Best validation accuracy: 62.01 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQP6PMinrOBV",
        "outputId": "b55348bd-dc56-4a76-f5d5-c8f2428f1c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results:\n",
            "  test accuracy:\t\t61.18 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwQM0saYTpy9"
      },
      "source": [
        "__Hint:__ If you don't want to compute shapes by hand, just plug in any shape (e.g. 1 unit) and run compute_loss. You will see something like this:\n",
        "\n",
        "__`RuntimeError: size mismatch, m1: [5 x 1960], m2: [1 x 64] at /some/long/path/to/torch/operation`__\n",
        "\n",
        "See the __1960__ there? That's your actual input shape.\n",
        "\n",
        "## Task 2: adding normalization\n",
        "\n",
        "* Add batch norm (with default params) between convolution and ReLU\n",
        "  * nn.BatchNorm*d (1d for dense, 2d for conv)\n",
        "  * usually better to put them after linear/conv but before nonlinearity\n",
        "* Re-train the network with the same optimizer, it should get at least 60% validation accuracy at peak.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 10, kernel_size=(3,3)),  # [10, 30, 30]\n",
        "    nn.BatchNorm2d(10),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2), # [10, 15, 15]\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(10 * 15 * 15, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(100, 10)\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "fg98W_YkVQBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(model, opt, X_train, y_train, X_val, y_val)"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8zIdovKscsG",
        "outputId": "4bdfc908-3611-4441-9514-b9bdd1d8d622"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100 took 1.536s\n",
            "  training loss (in-iteration): \t1.555549\n",
            "  validation accuracy: \t\t\t55.33 %\n",
            "Epoch 2 of 100 took 1.692s\n",
            "  training loss (in-iteration): \t1.264537\n",
            "  validation accuracy: \t\t\t54.60 %\n",
            "Epoch 3 of 100 took 1.734s\n",
            "  training loss (in-iteration): \t1.150061\n",
            "  validation accuracy: \t\t\t58.88 %\n",
            "Epoch 4 of 100 took 1.530s\n",
            "  training loss (in-iteration): \t1.077964\n",
            "  validation accuracy: \t\t\t60.75 %\n",
            "Epoch 5 of 100 took 1.526s\n",
            "  training loss (in-iteration): \t1.028120\n",
            "  validation accuracy: \t\t\t56.75 %\n",
            "Epoch 6 of 100 took 1.523s\n",
            "  training loss (in-iteration): \t0.981943\n",
            "  validation accuracy: \t\t\t59.98 %\n",
            "Epoch 7 of 100 took 1.571s\n",
            "  training loss (in-iteration): \t0.937545\n",
            "  validation accuracy: \t\t\t62.06 %\n",
            "Epoch 8 of 100 took 1.523s\n",
            "  training loss (in-iteration): \t0.907180\n",
            "  validation accuracy: \t\t\t60.76 %\n",
            "Epoch 9 of 100 took 1.508s\n",
            "  training loss (in-iteration): \t0.874411\n",
            "  validation accuracy: \t\t\t60.70 %\n",
            "Epoch 10 of 100 took 1.844s\n",
            "  training loss (in-iteration): \t0.841969\n",
            "  validation accuracy: \t\t\t59.11 %\n",
            "Epoch 11 of 100 took 1.624s\n",
            "  training loss (in-iteration): \t0.821243\n",
            "  validation accuracy: \t\t\t60.13 %\n",
            "Epoch 12 of 100 took 1.482s\n",
            "  training loss (in-iteration): \t0.793501\n",
            "  validation accuracy: \t\t\t58.51 %\n",
            "Epoch 13 of 100 took 1.527s\n",
            "  training loss (in-iteration): \t0.766398\n",
            "  validation accuracy: \t\t\t61.36 %\n",
            "Epoch 14 of 100 took 1.513s\n",
            "  training loss (in-iteration): \t0.748685\n",
            "  validation accuracy: \t\t\t60.74 %\n",
            "Epoch 15 of 100 took 1.511s\n",
            "  training loss (in-iteration): \t0.726986\n",
            "  validation accuracy: \t\t\t60.45 %\n",
            "Epoch 16 of 100 took 1.509s\n",
            "  training loss (in-iteration): \t0.705228\n",
            "  validation accuracy: \t\t\t60.86 %\n",
            "Epoch 17 of 100 took 1.645s\n",
            "  training loss (in-iteration): \t0.682150\n",
            "  validation accuracy: \t\t\t56.34 %\n",
            "Epoch 18 of 100 took 1.763s\n",
            "  training loss (in-iteration): \t0.676987\n",
            "  validation accuracy: \t\t\t60.85 %\n",
            "Epoch 19 of 100 took 1.521s\n",
            "  training loss (in-iteration): \t0.654396\n",
            "  validation accuracy: \t\t\t59.13 %\n",
            "Model did not see any loss improvements for 7 epochs, aborting...\n",
            "Finished training. Best validation accuracy: 62.06 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(model, X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJYqO-zqshBZ",
        "outputId": "5f04630a-3c85-4ae0-d02a-a042a733b3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results:\n",
            "  test accuracy:\t\t60.91 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB1f9ebCTpy9"
      },
      "source": [
        "## Task 3: Data Augmentation\n",
        "\n",
        "There's a powerful torch tool for image preprocessing useful to do data preprocessing and augmentation.\n",
        "\n",
        "Here's how it works: we define a pipeline that\n",
        "* makes random crops of data (augmentation)\n",
        "* randomly flips image horizontally (augmentation)\n",
        "* then normalizes it (preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "d8-OSeicTpy-"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "means = np.array((0.4914, 0.4822, 0.4465))  # statistics from dataset documentation\n",
        "stds = np.array((0.2023, 0.1994, 0.2010))\n",
        "\n",
        "transform_augment = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation([-30, 30]),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "a21LpvXxTpy-"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_val_dataset(dataset, val_split=0.2):\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_split)\n",
        "    datasets = {}\n",
        "    datasets['train'] = Subset(dataset, train_idx)\n",
        "    datasets['val'] = Subset(dataset, val_idx)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "train_dataset = CIFAR10(\"./cifar_data/\", train=True, transform=transform_augment)\n",
        "\n",
        "train_sets = train_val_dataset(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_upgraded(model, opt, train_dataset, val_dataset, num_epochs:int = 100, batch_size:int = 64, stop:int = 7, device:str = None):\n",
        "    \"\"\"\n",
        "    A function for training torch models that I kindly took from the professor (sorry) but now with torch dataloaders\n",
        "\n",
        "    Args:\n",
        "        model: torch model to train\n",
        "        opt: optimizer for that model\n",
        "        X_train: training data [n, m], where n is the number of entries and m --- the number of features\n",
        "        y_train: training targets [n,]\n",
        "        X_val: validation data\n",
        "        y_val: validation targets\n",
        "        num_epochs: total amount of full passes over training data (default: 100)\n",
        "        batch_size: number of samples processed in one SGD iteration (default: 64)\n",
        "        stop: number of iterations that loss can decrease for before the training proces stops (default: 7)\n",
        "    \"\"\"\n",
        "    train_loss = []\n",
        "    val_accuracy = []\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    val_dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # In each epoch, we do a full pass over the training data:\n",
        "        start_time = time.time()\n",
        "        model.train(True) # enable dropout / batch_norm training behavior\n",
        "        for (X_batch, y_batch) in train_dataloader:\n",
        "            X_batch = X_batch.to(torch.float32).to(device)\n",
        "            y_batch = y_batch.to(torch.int64).to(device)\n",
        "            # train on batch\n",
        "            logits = model(X_batch)\n",
        "            loss = F.cross_entropy(logits, y_batch).mean()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "        # And a full pass over the validation data:\n",
        "        model.train(False)     # disable dropout / use averages for batch_norm\n",
        "        with torch.no_grad():  # do not store intermediate activations\n",
        "            for (X_batch, y_batch) in val_dataloader:\n",
        "                X_batch = X_batch.to(torch.float32).to(device)\n",
        "                y_batch = y_batch.detach().cpu().numpy()\n",
        "                logits = model(X_batch)\n",
        "                y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
        "                val_accuracy.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "        mean_val_acc = np.mean(val_accuracy[-len(val_dataset) // batch_size :])\n",
        "\n",
        "        if best_val_acc < mean_val_acc:\n",
        "            best_val_acc = mean_val_acc\n",
        "            best_epoch = i\n",
        "            pathlib.Path(\"./models/\").mkdir(exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"models/best_model.pt2\")\n",
        "\n",
        "\n",
        "        if epoch - best_epoch > stop:\n",
        "            print(\"Model did not see any loss improvements for %i epochs, aborting...\" % (stop))\n",
        "            model.load_state_dict(torch.load(f\"models/best_model.pt2\", weights_only=True))\n",
        "            model.eval()\n",
        "            break\n",
        "\n",
        "        # Then we print the results for this epoch:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "            np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
        "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "            mean_val_acc * 100))\n",
        "\n",
        "    print(f\"Finished training. Best validation accuracy: {best_val_acc*100:.2f} %\")"
      ],
      "metadata": {
        "id": "BhUK5AIV0oNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_updated(model, test_dataset):\n",
        "    model.train(False) # disable dropout / use averages for batch_norm\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=500, shuffle=True, num_workers=1)\n",
        "    test_batch_acc = []\n",
        "    for X_batch, y_batch in test_dataloader:\n",
        "        X_batch = X_batch.to(torch.float32).to(device)\n",
        "        y_batch = y_batch.detach().cpu().numpy()\n",
        "        logits = model(X_batch)\n",
        "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
        "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "    test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "        test_accuracy * 100))"
      ],
      "metadata": {
        "id": "EhqMdrWG_8gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FZfWqT8pTpy-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92c18b6e-289b-4f9d-cb57-7dd37fa7ca18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100 took 24.999s\n",
            "  training loss (in-iteration): \t1.788986\n",
            "  validation accuracy: \t\t\t40.60 %\n",
            "Epoch 2 of 100 took 25.135s\n",
            "  training loss (in-iteration): \t1.604469\n",
            "  validation accuracy: \t\t\t44.21 %\n",
            "Epoch 3 of 100 took 25.115s\n",
            "  training loss (in-iteration): \t1.546970\n",
            "  validation accuracy: \t\t\t44.31 %\n",
            "Epoch 4 of 100 took 24.859s\n",
            "  training loss (in-iteration): \t1.523269\n",
            "  validation accuracy: \t\t\t46.31 %\n",
            "Epoch 5 of 100 took 25.222s\n",
            "  training loss (in-iteration): \t1.498357\n",
            "  validation accuracy: \t\t\t47.24 %\n",
            "Epoch 6 of 100 took 24.890s\n",
            "  training loss (in-iteration): \t1.489475\n",
            "  validation accuracy: \t\t\t47.65 %\n",
            "Epoch 7 of 100 took 25.788s\n",
            "  training loss (in-iteration): \t1.472225\n",
            "  validation accuracy: \t\t\t47.55 %\n",
            "Epoch 8 of 100 took 24.766s\n",
            "  training loss (in-iteration): \t1.465172\n",
            "  validation accuracy: \t\t\t48.51 %\n",
            "Epoch 9 of 100 took 25.318s\n",
            "  training loss (in-iteration): \t1.449785\n",
            "  validation accuracy: \t\t\t47.58 %\n",
            "Epoch 10 of 100 took 25.291s\n",
            "  training loss (in-iteration): \t1.440341\n",
            "  validation accuracy: \t\t\t48.66 %\n",
            "Epoch 11 of 100 took 25.317s\n",
            "  training loss (in-iteration): \t1.434301\n",
            "  validation accuracy: \t\t\t49.28 %\n",
            "Epoch 12 of 100 took 25.538s\n",
            "  training loss (in-iteration): \t1.423767\n",
            "  validation accuracy: \t\t\t50.16 %\n",
            "Epoch 13 of 100 took 24.843s\n",
            "  training loss (in-iteration): \t1.423630\n",
            "  validation accuracy: \t\t\t50.11 %\n",
            "Epoch 14 of 100 took 25.187s\n",
            "  training loss (in-iteration): \t1.418283\n",
            "  validation accuracy: \t\t\t49.99 %\n",
            "Epoch 15 of 100 took 25.305s\n",
            "  training loss (in-iteration): \t1.411905\n",
            "  validation accuracy: \t\t\t50.94 %\n",
            "Model did not see any loss improvements for 7 epochs, aborting...\n",
            "Finished training. Best validation accuracy: 50.94 %\n"
          ]
        }
      ],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 10, kernel_size=(3,3)),  # [10, 30, 30]\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(2), # [10, 15, 15]\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(10 * 15 * 15, 100),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(100, 10)\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_upgraded(model, opt, train_sets[\"train\"], train_sets[\"val\"], device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv2pU1HJTpy_"
      },
      "source": [
        "When testing, we don't need random crops, just normalize with same statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HgS33Q-5Tpy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21ffcab4-915d-4ba7-f9ca-eb56576c3888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final results:\n",
            "  test accuracy:\t\t51.23 %\n"
          ]
        }
      ],
      "source": [
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])\n",
        "\n",
        "\n",
        "test_dataset = CIFAR10(\"./cifar_data/\", train=False, transform=transform_test)\n",
        "evaluate_updated(model, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have got a baseline of 50%, let us improve it! First, let us change all the non-linearities to GELU:"
      ],
      "metadata": {
        "id": "1KEnDDenDGLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 10, kernel_size=(3,3)),  # [10, 30, 30]\n",
        "    nn.GELU(),\n",
        "    nn.MaxPool2d(2), # [10, 15, 15]\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(10 * 15 * 15, 100),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(100, 10)\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train_upgraded(model, opt, train_sets[\"train\"], train_sets[\"val\"], device=device, stop=10)\n",
        "evaluate_updated(model, test_dataset)"
      ],
      "metadata": {
        "id": "-SJOkw-gDIyb",
        "outputId": "ebe0ba5e-52d6-47ae-8036-413378246552",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 100 took 25.028s\n",
            "  training loss (in-iteration): \t1.738346\n",
            "  validation accuracy: \t\t\t43.20 %\n",
            "Epoch 2 of 100 took 24.981s\n",
            "  training loss (in-iteration): \t1.557186\n",
            "  validation accuracy: \t\t\t45.34 %\n",
            "Epoch 3 of 100 took 25.807s\n",
            "  training loss (in-iteration): \t1.489758\n",
            "  validation accuracy: \t\t\t47.05 %\n",
            "Epoch 4 of 100 took 24.886s\n",
            "  training loss (in-iteration): \t1.458499\n",
            "  validation accuracy: \t\t\t48.41 %\n",
            "Epoch 5 of 100 took 24.734s\n",
            "  training loss (in-iteration): \t1.435072\n",
            "  validation accuracy: \t\t\t49.45 %\n",
            "Epoch 6 of 100 took 24.807s\n",
            "  training loss (in-iteration): \t1.417875\n",
            "  validation accuracy: \t\t\t49.97 %\n",
            "Epoch 7 of 100 took 25.296s\n",
            "  training loss (in-iteration): \t1.400148\n",
            "  validation accuracy: \t\t\t50.14 %\n",
            "Epoch 8 of 100 took 25.366s\n",
            "  training loss (in-iteration): \t1.385419\n",
            "  validation accuracy: \t\t\t51.54 %\n",
            "Epoch 9 of 100 took 25.656s\n",
            "  training loss (in-iteration): \t1.373466\n",
            "  validation accuracy: \t\t\t52.10 %\n",
            "Epoch 10 of 100 took 25.107s\n",
            "  training loss (in-iteration): \t1.363951\n",
            "  validation accuracy: \t\t\t51.49 %\n",
            "Epoch 11 of 100 took 25.235s\n",
            "  training loss (in-iteration): \t1.359169\n",
            "  validation accuracy: \t\t\t52.48 %\n",
            "Epoch 12 of 100 took 25.336s\n",
            "  training loss (in-iteration): \t1.347336\n",
            "  validation accuracy: \t\t\t52.08 %\n",
            "Epoch 13 of 100 took 26.345s\n",
            "  training loss (in-iteration): \t1.345798\n",
            "  validation accuracy: \t\t\t52.42 %\n",
            "Epoch 14 of 100 took 25.303s\n",
            "  training loss (in-iteration): \t1.329378\n",
            "  validation accuracy: \t\t\t52.52 %\n",
            "Epoch 15 of 100 took 24.985s\n",
            "  training loss (in-iteration): \t1.329011\n",
            "  validation accuracy: \t\t\t53.01 %\n",
            "Epoch 16 of 100 took 25.012s\n",
            "  training loss (in-iteration): \t1.325248\n",
            "  validation accuracy: \t\t\t53.58 %\n",
            "Epoch 17 of 100 took 24.717s\n",
            "  training loss (in-iteration): \t1.315838\n",
            "  validation accuracy: \t\t\t54.50 %\n",
            "Epoch 18 of 100 took 24.764s\n",
            "  training loss (in-iteration): \t1.313828\n",
            "  validation accuracy: \t\t\t54.38 %\n",
            "Model did not see any loss improvements for 10 epochs, aborting...\n",
            "Finished training. Best validation accuracy: 54.50 %\n",
            "Final results:\n",
            "  test accuracy:\t\t55.11 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}