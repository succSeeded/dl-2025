{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/succSeeded/dl-2025/blob/main/hws/week03_convnets/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUpNZCOMTpy_"
   },
   "source": [
    "# Homework 2.2: The Quest For A Better Network\n",
    "\n",
    "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
    "\n",
    "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL6UHGwcTpy_"
   },
   "source": [
    "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
    "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
    "\n",
    "## Grading\n",
    "* starting at zero points\n",
    "* +20% for describing your iteration path in a report below.\n",
    "* +20% for building a network that gets above 20% accuracy\n",
    "* +10% for beating each of these milestones on __TEST__ dataset:\n",
    "    * 50% (50% points)\n",
    "    * 60% (60% points)\n",
    "    * 65% (70% points)\n",
    "    * 70% (80% points)\n",
    "    * 75% (90% points)\n",
    "    * 80% (full points)\n",
    "    \n",
    "## Restrictions\n",
    "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
    "* In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
    "* you __can__ use validation data for training, but you __can't__ do anything with test data apart from running the evaluation procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL6UHGwcTpy_"
   },
   "source": [
    "## Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OL6UHGwcTpy_"
   },
   "source": [
    "### Network size and architecture\n",
    "For this task I decided to try out multiple architectures:\n",
    "* a network proposed in out class as a baseline;\n",
    "* `AlexNet`;\n",
    "* `Resnet-18` (since bigger options might be an overkill).\n",
    "\n",
    "### Optimizations\n",
    "The training process included the following optimizations:\n",
    "* `Adam` optimizer was used since it is stronger, faster and better than `SGD`, althouhg its parameters values were default for each test case;\n",
    "* a stopping criterion that terminates the training process after a certain amount of epochs without improvements on global optimum (10 each case);\n",
    "* a dropout layer with `p = 0.1` for AlexNet and baseline architectures;\n",
    "\n",
    "   \n",
    "### Data augmemntation\n",
    "For each test case the data was augmente using the procedure described in out class: \n",
    "```\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3f5pL9sXTpzA"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_val_split(dataset, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Split torch datasets into train and validation parts\n",
    "\n",
    "    Args:\n",
    "        dataset: incoming torch.Dataset object\n",
    "        val_size: portion of the dataset that will be used for validation (default: 0.2)\n",
    "    \"\"\"\n",
    "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_size)\n",
    "    return (Subset(dataset, train_idx), Subset(dataset, val_idx))\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "means = np.array((0.4914, 0.4822, 0.4465))  # statistics from dataset documentation\n",
    "stds = np.array((0.2023, 0.1994, 0.2010))\n",
    "\n",
    "transform_augment = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomRotation([-30, 30]),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(means, stds),\n",
    "])\n",
    "\n",
    "\n",
    "test_dataset = CIFAR10(\"./data/\", train=False, download=True, transform=transform_test)\n",
    "train_val_dataset = CIFAR10(\"./data/\", train=True, download=True, transform=transform_augment)\n",
    "\n",
    "train_dataset, val_dataset = train_val_split(train_val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "BhUK5AIV0oNS"
   },
   "outputs": [],
   "source": [
    "def train(model, opt, train_dataset, val_dataset, num_epochs:int = 100, batch_size:int = 64, stop:int = 7, device:str = None):\n",
    "    \"\"\"\n",
    "    A function for training torch models that I kindly took from the professor (sorry) but now with torch dataloaders\n",
    "\n",
    "    Args:\n",
    "        model: torch model to train\n",
    "        opt: optimizer for that model\n",
    "        train_dataset: torch.Dataset used for training\n",
    "        val_dataset: torch.Dataset used for validation\n",
    "        num_epochs: total amount of full passes over training data (default: 100)\n",
    "        batch_size: number of samples processed in one SGD iteration (default: 64)\n",
    "        stop: number of iterations that loss can decrease for before the training proces stops (default: 7)\n",
    "    \"\"\"\n",
    "    train_loss = []\n",
    "    val_accuracy = []\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # In each epoch, we do a full pass over the training data:\n",
    "        start_time = time.time()\n",
    "        model.train(True) # enable dropout / batch_norm training behavior\n",
    "        for (X_batch, y_batch) in train_dataloader:\n",
    "            X_batch = X_batch.to(torch.float32).to(device)\n",
    "            y_batch = y_batch.to(torch.int64).to(device)\n",
    "            # train on batch\n",
    "            logits = model(X_batch)\n",
    "            loss = F.cross_entropy(logits, y_batch).mean()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "            train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
    "\n",
    "        # And a full pass over the validation data:\n",
    "        model.train(False)     # disable dropout / use averages for batch_norm\n",
    "        with torch.no_grad():  # do not store intermediate activations\n",
    "            for (X_batch, y_batch) in val_dataloader:\n",
    "                X_batch = X_batch.to(torch.float32).to(device)\n",
    "                y_batch = y_batch.detach().cpu().numpy()\n",
    "                logits = model(X_batch)\n",
    "                y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
    "                val_accuracy.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "        mean_val_acc = np.mean(val_accuracy[-len(val_dataset) // batch_size :])\n",
    "\n",
    "        if best_val_acc < mean_val_acc:\n",
    "            best_val_acc = mean_val_acc\n",
    "            best_epoch = epoch\n",
    "            pathlib.Path(\"./models/\").mkdir(exist_ok=True)\n",
    "            torch.save(model.state_dict(), f\"models/best_model.pt2\")\n",
    "\n",
    "\n",
    "        if epoch - best_epoch > stop:\n",
    "            print(\"Model did not see any loss improvements for %i epochs, aborting...\" % (stop))\n",
    "            model.load_state_dict(torch.load(f\"models/best_model.pt2\", weights_only=True))\n",
    "            model.eval()\n",
    "            break\n",
    "\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch + 1, num_epochs, time.time() - start_time))\n",
    "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
    "            np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
    "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
    "            mean_val_acc * 100))\n",
    "\n",
    "    print(f\"Finished training. Best validation accuracy: {best_val_acc*100:.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EhqMdrWG_8gO"
   },
   "outputs": [],
   "source": [
    "def evaluate(model, test_dataset):\n",
    "    \"\"\"\n",
    "    A function for evaluating torch models but now with torch dataloaders\n",
    "\n",
    "    Args:\n",
    "        model: torch model to evaluate\n",
    "        test_dataset: torch.Dataset that contains test data\n",
    "    \"\"\"\n",
    "    model.train(False) # disable dropout / use averages for batch_norm\n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=500, shuffle=True, num_workers=1)\n",
    "    test_batch_acc = []\n",
    "\n",
    "    for X_batch, y_batch in test_dataloader:\n",
    "        X_batch = X_batch.to(torch.float32).to(device)\n",
    "        y_batch = y_batch.detach().cpu().numpy()\n",
    "        logits = model(X_batch)\n",
    "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
    "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
    "\n",
    "    test_accuracy = np.mean(test_batch_acc)\n",
    "\n",
    "    print(\"Final results:\")\n",
    "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
    "        test_accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNetBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3,3), stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3,3), stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels or stride != 1:\n",
    "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=stride, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        inputs = self.shortcut(x)\n",
    "\n",
    "        out = self.gelu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += inputs\n",
    "        out = self.gelu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Z3FnyxjarXcL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 100 took 10.047s\n",
      "  training loss (in-iteration): \t2.205096\n",
      "  validation accuracy: \t\t\t25.89 %\n",
      "Epoch 2 of 100 took 9.812s\n",
      "  training loss (in-iteration): \t2.164611\n",
      "  validation accuracy: \t\t\t31.33 %\n",
      "Epoch 3 of 100 took 9.807s\n",
      "  training loss (in-iteration): \t2.150024\n",
      "  validation accuracy: \t\t\t30.58 %\n",
      "Epoch 4 of 100 took 9.845s\n",
      "  training loss (in-iteration): \t2.126246\n",
      "  validation accuracy: \t\t\t36.62 %\n",
      "Epoch 5 of 100 took 9.859s\n",
      "  training loss (in-iteration): \t2.083313\n",
      "  validation accuracy: \t\t\t36.65 %\n",
      "Epoch 6 of 100 took 9.893s\n",
      "  training loss (in-iteration): \t2.062417\n",
      "  validation accuracy: \t\t\t41.25 %\n",
      "Epoch 7 of 100 took 9.933s\n",
      "  training loss (in-iteration): \t2.028416\n",
      "  validation accuracy: \t\t\t42.88 %\n",
      "Epoch 8 of 100 took 9.967s\n",
      "  training loss (in-iteration): \t1.996588\n",
      "  validation accuracy: \t\t\t46.53 %\n",
      "Epoch 9 of 100 took 9.962s\n",
      "  training loss (in-iteration): \t1.969027\n",
      "  validation accuracy: \t\t\t48.88 %\n",
      "Epoch 10 of 100 took 9.940s\n",
      "  training loss (in-iteration): \t1.946865\n",
      "  validation accuracy: \t\t\t51.31 %\n",
      "Epoch 11 of 100 took 9.926s\n",
      "  training loss (in-iteration): \t1.928607\n",
      "  validation accuracy: \t\t\t52.71 %\n",
      "Epoch 12 of 100 took 9.949s\n",
      "  training loss (in-iteration): \t1.914335\n",
      "  validation accuracy: \t\t\t53.41 %\n",
      "Epoch 13 of 100 took 9.951s\n",
      "  training loss (in-iteration): \t1.899457\n",
      "  validation accuracy: \t\t\t54.38 %\n",
      "Epoch 14 of 100 took 9.954s\n",
      "  training loss (in-iteration): \t1.888379\n",
      "  validation accuracy: \t\t\t55.36 %\n",
      "Epoch 15 of 100 took 9.923s\n",
      "  training loss (in-iteration): \t1.879141\n",
      "  validation accuracy: \t\t\t57.92 %\n",
      "Epoch 16 of 100 took 9.978s\n",
      "  training loss (in-iteration): \t1.870371\n",
      "  validation accuracy: \t\t\t58.80 %\n",
      "Epoch 17 of 100 took 9.968s\n",
      "  training loss (in-iteration): \t1.859550\n",
      "  validation accuracy: \t\t\t58.94 %\n",
      "Epoch 18 of 100 took 9.924s\n",
      "  training loss (in-iteration): \t1.852897\n",
      "  validation accuracy: \t\t\t59.78 %\n",
      "Epoch 19 of 100 took 9.936s\n",
      "  training loss (in-iteration): \t1.846853\n",
      "  validation accuracy: \t\t\t60.79 %\n",
      "Epoch 20 of 100 took 9.922s\n",
      "  training loss (in-iteration): \t1.834239\n",
      "  validation accuracy: \t\t\t60.82 %\n",
      "Epoch 21 of 100 took 9.952s\n",
      "  training loss (in-iteration): \t1.827505\n",
      "  validation accuracy: \t\t\t62.98 %\n",
      "Epoch 22 of 100 took 9.946s\n",
      "  training loss (in-iteration): \t1.824271\n",
      "  validation accuracy: \t\t\t64.10 %\n",
      "Epoch 23 of 100 took 9.948s\n",
      "  training loss (in-iteration): \t1.815731\n",
      "  validation accuracy: \t\t\t63.01 %\n",
      "Epoch 24 of 100 took 9.914s\n",
      "  training loss (in-iteration): \t1.814958\n",
      "  validation accuracy: \t\t\t63.51 %\n",
      "Epoch 25 of 100 took 9.961s\n",
      "  training loss (in-iteration): \t1.803920\n",
      "  validation accuracy: \t\t\t64.68 %\n",
      "Epoch 26 of 100 took 9.911s\n",
      "  training loss (in-iteration): \t1.801909\n",
      "  validation accuracy: \t\t\t64.64 %\n",
      "Epoch 27 of 100 took 9.939s\n",
      "  training loss (in-iteration): \t1.790320\n",
      "  validation accuracy: \t\t\t65.00 %\n",
      "Epoch 28 of 100 took 9.931s\n",
      "  training loss (in-iteration): \t1.790235\n",
      "  validation accuracy: \t\t\t65.09 %\n",
      "Epoch 29 of 100 took 9.950s\n",
      "  training loss (in-iteration): \t1.787492\n",
      "  validation accuracy: \t\t\t65.50 %\n",
      "Epoch 30 of 100 took 9.944s\n",
      "  training loss (in-iteration): \t1.776025\n",
      "  validation accuracy: \t\t\t66.41 %\n",
      "Epoch 31 of 100 took 9.937s\n",
      "  training loss (in-iteration): \t1.775668\n",
      "  validation accuracy: \t\t\t66.90 %\n",
      "Epoch 32 of 100 took 9.948s\n",
      "  training loss (in-iteration): \t1.774765\n",
      "  validation accuracy: \t\t\t67.14 %\n",
      "Epoch 33 of 100 took 9.974s\n",
      "  training loss (in-iteration): \t1.770363\n",
      "  validation accuracy: \t\t\t67.58 %\n",
      "Epoch 34 of 100 took 9.933s\n",
      "  training loss (in-iteration): \t1.765841\n",
      "  validation accuracy: \t\t\t67.85 %\n",
      "Epoch 35 of 100 took 9.945s\n",
      "  training loss (in-iteration): \t1.764966\n",
      "  validation accuracy: \t\t\t68.01 %\n",
      "Epoch 36 of 100 took 9.903s\n",
      "  training loss (in-iteration): \t1.757534\n",
      "  validation accuracy: \t\t\t68.52 %\n",
      "Epoch 37 of 100 took 9.945s\n",
      "  training loss (in-iteration): \t1.757133\n",
      "  validation accuracy: \t\t\t67.41 %\n",
      "Epoch 38 of 100 took 9.939s\n",
      "  training loss (in-iteration): \t1.751709\n",
      "  validation accuracy: \t\t\t68.82 %\n",
      "Epoch 39 of 100 took 9.931s\n",
      "  training loss (in-iteration): \t1.748485\n",
      "  validation accuracy: \t\t\t69.23 %\n",
      "Epoch 40 of 100 took 9.935s\n",
      "  training loss (in-iteration): \t1.744151\n",
      "  validation accuracy: \t\t\t70.54 %\n",
      "Epoch 41 of 100 took 9.920s\n",
      "  training loss (in-iteration): \t1.741201\n",
      "  validation accuracy: \t\t\t70.62 %\n",
      "Epoch 42 of 100 took 9.943s\n",
      "  training loss (in-iteration): \t1.740705\n",
      "  validation accuracy: \t\t\t71.30 %\n",
      "Epoch 43 of 100 took 9.943s\n",
      "  training loss (in-iteration): \t1.735989\n",
      "  validation accuracy: \t\t\t70.78 %\n",
      "Epoch 44 of 100 took 9.917s\n",
      "  training loss (in-iteration): \t1.736507\n",
      "  validation accuracy: \t\t\t71.16 %\n",
      "Epoch 45 of 100 took 9.925s\n",
      "  training loss (in-iteration): \t1.732614\n",
      "  validation accuracy: \t\t\t70.89 %\n",
      "Epoch 46 of 100 took 9.947s\n",
      "  training loss (in-iteration): \t1.733198\n",
      "  validation accuracy: \t\t\t71.45 %\n",
      "Epoch 47 of 100 took 9.963s\n",
      "  training loss (in-iteration): \t1.724767\n",
      "  validation accuracy: \t\t\t72.36 %\n",
      "Epoch 48 of 100 took 9.964s\n",
      "  training loss (in-iteration): \t1.723845\n",
      "  validation accuracy: \t\t\t71.32 %\n",
      "Epoch 49 of 100 took 9.913s\n",
      "  training loss (in-iteration): \t1.722150\n",
      "  validation accuracy: \t\t\t71.60 %\n",
      "Epoch 50 of 100 took 9.930s\n",
      "  training loss (in-iteration): \t1.725234\n",
      "  validation accuracy: \t\t\t71.88 %\n",
      "Epoch 51 of 100 took 9.911s\n",
      "  training loss (in-iteration): \t1.717963\n",
      "  validation accuracy: \t\t\t72.68 %\n",
      "Epoch 52 of 100 took 9.963s\n",
      "  training loss (in-iteration): \t1.715585\n",
      "  validation accuracy: \t\t\t73.35 %\n",
      "Epoch 53 of 100 took 9.921s\n",
      "  training loss (in-iteration): \t1.715401\n",
      "  validation accuracy: \t\t\t73.75 %\n",
      "Epoch 54 of 100 took 9.932s\n",
      "  training loss (in-iteration): \t1.711943\n",
      "  validation accuracy: \t\t\t72.05 %\n",
      "Epoch 55 of 100 took 9.943s\n",
      "  training loss (in-iteration): \t1.711273\n",
      "  validation accuracy: \t\t\t72.74 %\n",
      "Epoch 56 of 100 took 9.935s\n",
      "  training loss (in-iteration): \t1.711349\n",
      "  validation accuracy: \t\t\t73.77 %\n",
      "Epoch 57 of 100 took 9.902s\n",
      "  training loss (in-iteration): \t1.712221\n",
      "  validation accuracy: \t\t\t72.79 %\n",
      "Epoch 58 of 100 took 9.931s\n",
      "  training loss (in-iteration): \t1.708620\n",
      "  validation accuracy: \t\t\t72.60 %\n",
      "Epoch 59 of 100 took 9.909s\n",
      "  training loss (in-iteration): \t1.704059\n",
      "  validation accuracy: \t\t\t73.56 %\n",
      "Epoch 60 of 100 took 9.943s\n",
      "  training loss (in-iteration): \t1.703596\n",
      "  validation accuracy: \t\t\t73.72 %\n",
      "Epoch 61 of 100 took 9.926s\n",
      "  training loss (in-iteration): \t1.703340\n",
      "  validation accuracy: \t\t\t73.66 %\n",
      "Epoch 62 of 100 took 9.958s\n",
      "  training loss (in-iteration): \t1.699987\n",
      "  validation accuracy: \t\t\t73.76 %\n",
      "Epoch 63 of 100 took 9.940s\n",
      "  training loss (in-iteration): \t1.699789\n",
      "  validation accuracy: \t\t\t74.28 %\n",
      "Epoch 64 of 100 took 10.001s\n",
      "  training loss (in-iteration): \t1.700993\n",
      "  validation accuracy: \t\t\t74.36 %\n",
      "Epoch 65 of 100 took 9.920s\n",
      "  training loss (in-iteration): \t1.698108\n",
      "  validation accuracy: \t\t\t73.82 %\n",
      "Epoch 66 of 100 took 9.941s\n",
      "  training loss (in-iteration): \t1.698521\n",
      "  validation accuracy: \t\t\t74.04 %\n",
      "Epoch 67 of 100 took 9.950s\n",
      "  training loss (in-iteration): \t1.696616\n",
      "  validation accuracy: \t\t\t73.89 %\n",
      "Epoch 68 of 100 took 9.930s\n",
      "  training loss (in-iteration): \t1.691967\n",
      "  validation accuracy: \t\t\t75.34 %\n",
      "Epoch 69 of 100 took 9.941s\n",
      "  training loss (in-iteration): \t1.694029\n",
      "  validation accuracy: \t\t\t73.89 %\n",
      "Epoch 70 of 100 took 9.934s\n",
      "  training loss (in-iteration): \t1.693480\n",
      "  validation accuracy: \t\t\t74.17 %\n",
      "Epoch 71 of 100 took 9.960s\n",
      "  training loss (in-iteration): \t1.690947\n",
      "  validation accuracy: \t\t\t74.43 %\n",
      "Epoch 72 of 100 took 9.904s\n",
      "  training loss (in-iteration): \t1.688234\n",
      "  validation accuracy: \t\t\t74.71 %\n",
      "Epoch 73 of 100 took 9.949s\n",
      "  training loss (in-iteration): \t1.685197\n",
      "  validation accuracy: \t\t\t75.51 %\n",
      "Epoch 74 of 100 took 9.924s\n",
      "  training loss (in-iteration): \t1.685863\n",
      "  validation accuracy: \t\t\t75.58 %\n",
      "Epoch 75 of 100 took 9.933s\n",
      "  training loss (in-iteration): \t1.685168\n",
      "  validation accuracy: \t\t\t75.84 %\n",
      "Epoch 76 of 100 took 9.943s\n",
      "  training loss (in-iteration): \t1.682677\n",
      "  validation accuracy: \t\t\t75.20 %\n",
      "Epoch 77 of 100 took 9.942s\n",
      "  training loss (in-iteration): \t1.684903\n",
      "  validation accuracy: \t\t\t74.93 %\n",
      "Epoch 78 of 100 took 9.940s\n",
      "  training loss (in-iteration): \t1.681467\n",
      "  validation accuracy: \t\t\t75.23 %\n",
      "Epoch 79 of 100 took 9.928s\n",
      "  training loss (in-iteration): \t1.681877\n",
      "  validation accuracy: \t\t\t75.48 %\n",
      "Epoch 80 of 100 took 9.951s\n",
      "  training loss (in-iteration): \t1.677994\n",
      "  validation accuracy: \t\t\t75.26 %\n",
      "Epoch 81 of 100 took 9.924s\n",
      "  training loss (in-iteration): \t1.677302\n",
      "  validation accuracy: \t\t\t75.71 %\n",
      "Epoch 82 of 100 took 9.948s\n",
      "  training loss (in-iteration): \t1.678696\n",
      "  validation accuracy: \t\t\t75.88 %\n",
      "Epoch 83 of 100 took 9.921s\n",
      "  training loss (in-iteration): \t1.673464\n",
      "  validation accuracy: \t\t\t75.78 %\n",
      "Epoch 84 of 100 took 9.931s\n",
      "  training loss (in-iteration): \t1.673520\n",
      "  validation accuracy: \t\t\t76.27 %\n",
      "Epoch 85 of 100 took 9.939s\n",
      "  training loss (in-iteration): \t1.675567\n",
      "  validation accuracy: \t\t\t75.78 %\n",
      "Epoch 86 of 100 took 9.928s\n",
      "  training loss (in-iteration): \t1.672063\n",
      "  validation accuracy: \t\t\t76.24 %\n",
      "Epoch 87 of 100 took 9.940s\n",
      "  training loss (in-iteration): \t1.672179\n",
      "  validation accuracy: \t\t\t75.80 %\n",
      "Epoch 88 of 100 took 9.945s\n",
      "  training loss (in-iteration): \t1.671978\n",
      "  validation accuracy: \t\t\t76.10 %\n",
      "Epoch 89 of 100 took 9.946s\n",
      "  training loss (in-iteration): \t1.672218\n",
      "  validation accuracy: \t\t\t76.69 %\n",
      "Epoch 90 of 100 took 9.929s\n",
      "  training loss (in-iteration): \t1.668913\n",
      "  validation accuracy: \t\t\t76.86 %\n",
      "Epoch 91 of 100 took 9.933s\n",
      "  training loss (in-iteration): \t1.670059\n",
      "  validation accuracy: \t\t\t76.43 %\n",
      "Epoch 92 of 100 took 9.928s\n",
      "  training loss (in-iteration): \t1.668329\n",
      "  validation accuracy: \t\t\t76.27 %\n",
      "Epoch 93 of 100 took 9.910s\n",
      "  training loss (in-iteration): \t1.669201\n",
      "  validation accuracy: \t\t\t76.94 %\n",
      "Epoch 94 of 100 took 9.937s\n",
      "  training loss (in-iteration): \t1.666340\n",
      "  validation accuracy: \t\t\t76.33 %\n",
      "Epoch 95 of 100 took 9.935s\n",
      "  training loss (in-iteration): \t1.665529\n",
      "  validation accuracy: \t\t\t76.46 %\n",
      "Epoch 96 of 100 took 9.947s\n",
      "  training loss (in-iteration): \t1.663695\n",
      "  validation accuracy: \t\t\t77.87 %\n",
      "Epoch 97 of 100 took 9.934s\n",
      "  training loss (in-iteration): \t1.665666\n",
      "  validation accuracy: \t\t\t77.18 %\n",
      "Epoch 98 of 100 took 9.952s\n",
      "  training loss (in-iteration): \t1.664208\n",
      "  validation accuracy: \t\t\t77.01 %\n",
      "Epoch 99 of 100 took 9.941s\n",
      "  training loss (in-iteration): \t1.660685\n",
      "  validation accuracy: \t\t\t77.32 %\n",
      "Epoch 100 of 100 took 9.959s\n",
      "  training loss (in-iteration): \t1.660565\n",
      "  validation accuracy: \t\t\t76.89 %\n",
      "Finished training. Best validation accuracy: 77.87 %\n",
      "Final results:\n",
      "  test accuracy:\t\t80.41 %\n"
     ]
    }
   ],
   "source": [
    "# =========== Baseline neural network ==================\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(3, 10, kernel_size=(3,3)),  # [10, 30, 30]\n",
    "#     nn.GELU(),\n",
    "#     nn.MaxPool2d(2), # [10, 15, 15]\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(10 * 15 * 15, 100),\n",
    "#     nn.GELU(),\n",
    "#     nn.Dropout(p=0.1),\n",
    "#     nn.Linear(100, 10)\n",
    "# ).to(device) # this produces:\n",
    "#                     58.42% accuracy on test set, ~7.1s per epoch with GELU\n",
    "#                     55.48% accuracy on test set, ~6.9s per epoch with tanh\n",
    "#                     54.55% accuracy on test set, ~6.8s per epoch with ReLU\n",
    "\n",
    "# ================== AlexNet ===========================\n",
    "# model = nn.Sequential(\n",
    "#     nn.Conv2d(3, 16, kernel_size=(5,5)),  # [16, 28, 28]\n",
    "#     nn.ReLU(),\n",
    "#     nn.Conv2d(16, 64, kernel_size=(3,3)), # [64, 26, 26]\n",
    "#     nn.MaxPool2d(2), # [64, 13, 13]\n",
    "#     nn.Conv2d(64, 96, kernel_size=(3,3), padding=1), # [96, 13, 13]\n",
    "#     nn.Conv2d(96, 96, kernel_size=(3,3), padding=1), # [96, 13, 13]\n",
    "#     nn.Conv2d(96, 64, kernel_size=(3,3), padding=1), # [96, 13, 13]\n",
    "#     nn.MaxPool2d(2), # [64, 6, 6]\n",
    "#     nn.Flatten(),\n",
    "#     nn.Linear(64 * 6 * 6, 4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(4096, 4096),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Dropout(p=0.1),\n",
    "#     nn.Linear(4096, 10)\n",
    "# ).to(device) # this produces:\n",
    "# #                   71.36% accuracy on test set, ~12.5s per epoch with ReLU\n",
    "# #                   exploding everything if ReLUs are substituted with GELUs\n",
    "\n",
    "\n",
    "# ================== ResNet-18 =========================\n",
    "# 18 + 1 conv layers and 1 dense layer\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, kernel_size=(3,3), stride=1, padding=1),  # [16, 32, 32]\n",
    "    ResNetBlock(16, 16), # [16, 32, 32]\n",
    "    ResNetBlock(16, 16), # [16, 32, 32]\n",
    "    ResNetBlock(16, 16), # [16, 32, 32]\n",
    "    ResNetBlock(16, 32, stride=2), # [32, 16, 16]\n",
    "    ResNetBlock(32, 32), # [32, 16, 16]\n",
    "    ResNetBlock(32, 32), # [32, 16, 16]\n",
    "    ResNetBlock(32, 64, stride=2), # [64, 8, 8]\n",
    "    ResNetBlock(64, 64), # [64, 8, 8]\n",
    "    ResNetBlock(64, 64), # [64, 8, 8]\n",
    "    nn.AvgPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 8 * 8, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ").to(device)# this produces:\n",
    "#                  78.05% accuracy on test set, ~9.7s per epoch with ReLU\n",
    "#                  80.41% accuracy on test set, ~9.9s per epoch with ReLU\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train(model, opt, train_dataset, val_dataset, device=device, stop=10)\n",
    "evaluate(model, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "homework.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
