{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/succSeeded/dl-2025/blob/main/hws/week03_convnets/homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUpNZCOMTpy_"
      },
      "source": [
        "# Homework 2.2: The Quest For A Better Network\n",
        "\n",
        "In this assignment you will build a monster network to solve CIFAR10 image classification.\n",
        "\n",
        "This notebook is intended as a sequel to seminar 3, please give it a try if you haven't done so yet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL6UHGwcTpy_"
      },
      "source": [
        "(please read it at least diagonally)\n",
        "\n",
        "* The ultimate quest is to create a network that has as high __accuracy__ as you can push it.\n",
        "* There is a __mini-report__ at the end that you will have to fill in. We recommend reading it first and filling it while you iterate.\n",
        "\n",
        "## Grading\n",
        "* starting at zero points\n",
        "* +20% for describing your iteration path in a report below.\n",
        "* +20% for building a network that gets above 20% accuracy\n",
        "* +10% for beating each of these milestones on __TEST__ dataset:\n",
        "    * 50% (50% points)\n",
        "    * 60% (60% points)\n",
        "    * 65% (70% points)\n",
        "    * 70% (80% points)\n",
        "    * 75% (90% points)\n",
        "    * 80% (full points)\n",
        "    \n",
        "## Restrictions\n",
        "* Please do NOT use pre-trained networks for this assignment until you reach 80%.\n",
        " * In other words, base milestones must be beaten without pre-trained nets (and such net must be present in the e-mail). After that, you can use whatever you want.\n",
        "* you __can__ use validation data for training, but you __can't'__ do anything with test data apart from running the evaluation procedure.\n",
        "\n",
        "## Tips on what can be done:\n",
        "\n",
        "\n",
        " * __Network size__\n",
        "   * MOAR neurons,\n",
        "   * MOAR layers, ([torch.nn docs](http://pytorch.org/docs/master/nn.html))\n",
        "\n",
        "   * Nonlinearities in the hidden layers\n",
        "     * tanh, relu, leaky relu, etc\n",
        "   * Larger networks may take more epochs to train, so don't discard your net just because it could didn't beat the baseline in 5 epochs.\n",
        "\n",
        "   * Ph'nglui mglw'nafh Cthulhu R'lyeh wgah'nagl fhtagn!\n",
        "\n",
        "\n",
        "### The main rule of prototyping: one change at a time\n",
        "   * By now you probably have several ideas on what to change. By all means, try them out! But there's a catch: __never test several new things at once__.\n",
        "\n",
        "\n",
        "### Optimization\n",
        "   * Training for 100 epochs regardless of anything is probably a bad idea.\n",
        "   * Some networks converge over 5 epochs, others - over 500.\n",
        "   * Way to go: stop when validation score is 10 iterations past maximum\n",
        "   * You should certainly use adaptive optimizers\n",
        "     * rmsprop, nesterov_momentum, adam, adagrad and so on.\n",
        "     * Converge faster and sometimes reach better optima\n",
        "     * It might make sense to tweak learning rate/momentum, other learning parameters, batch size and number of epochs\n",
        "   * __BatchNormalization__ (nn.BatchNorm2d) for the win!\n",
        "     * Sometimes more batch normalization is better.\n",
        "   * __Regularize__ to prevent overfitting\n",
        "     * Add some L2 weight norm to the loss function, PyTorch will do the rest\n",
        "       * Can be done manually or with weight_decay parameter of a optimizer ([for example SGD's doc](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD)).\n",
        "     * Dropout (`nn.Dropout`) - to prevent overfitting\n",
        "       * Don't overdo it. Check if it actually makes your network better\n",
        "   \n",
        "### Convolution architectures\n",
        "   * This task __can__ be solved by a sequence of convolutions and poolings with batch_norm and ReLU seasoning, but you shouldn't necessarily stop there.\n",
        "   * [Inception family](https://hacktilldawn.com/2016/09/25/inception-modules-explained-and-implemented/), [ResNet family](https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035?gi=9018057983ca), [Densely-connected convolutions (exotic)](https://arxiv.org/abs/1608.06993), [Capsule networks (exotic)](https://arxiv.org/abs/1710.09829)\n",
        "   * Please do try a few simple architectures before you go for resnet-152.\n",
        "   * Warning! Training convolutional networks can take long without GPU. That's okay.\n",
        "     * If you are CPU-only, we still recomment that you try a simple convolutional architecture\n",
        "     * a perfect option is if you can set it up to run at nighttime and check it up at the morning.\n",
        "     * Make reasonable layer size estimates. A 128-neuron first convolution is likely an overkill.\n",
        "     * __To reduce computation__ time by a factor in exchange for some accuracy drop, try using __stride__ parameter. A stride=2 convolution should take roughly 1/4 of the default (stride=1) one.\n",
        "\n",
        "   \n",
        "### Data augmemntation\n",
        "   * getting 5x as large dataset for free is a great\n",
        "     * Zoom-in+slice = move\n",
        "     * Rotate+zoom(to remove black stripes)\n",
        "     * Add Noize (gaussian or bernoulli)\n",
        "   * Simple way to do that (if you have PIL/Image):\n",
        "     * ```from scipy.misc import imrotate,imresize```\n",
        "     * and a few slicing\n",
        "     * Other cool libraries: cv2, skimake, PIL/Pillow\n",
        "   * A more advanced way is to use torchvision transforms:\n",
        "```\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=path_to_cifar_like_in_seminar,\n",
        "        train=True,\n",
        "        download=True,\n",
        "        transform=transform_train\n",
        "    )\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        trainset,\n",
        "        batch_size=128,\n",
        "        shuffle=True,\n",
        "        num_workers=2\n",
        "    )\n",
        "```\n",
        "   * Or use this tool from Keras (requires theano/tensorflow): [tutorial](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), [docs](https://keras.io/preprocessing/image/)\n",
        "   * Stay realistic. There's usually no point in flipping dogs upside down as that is not the way you usually see them.\n",
        "   \n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3f5pL9sXTpzA"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import pathlib\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def train_val_split(dataset, val_size=0.2):\n",
        "    \"\"\"\n",
        "    Split torch datasets into train and validation parts\n",
        "\n",
        "    Args:\n",
        "        dataset: incoming torch.Dataset object\n",
        "        val_size: portion of the dataset that will be used for validation (default: 0.2)\n",
        "    \"\"\"\n",
        "    train_idx, val_idx = train_test_split(list(range(len(dataset))), test_size=val_size)\n",
        "    return (Subset(dataset, train_idx), Subset(dataset, val_idx))\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "means = np.array((0.4914, 0.4822, 0.4465))  # statistics from dataset documentation\n",
        "stds = np.array((0.2023, 0.1994, 0.2010))\n",
        "\n",
        "transform_augment = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomRotation([-30, 30]),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])\n",
        "\n",
        "\n",
        "test_dataset = CIFAR10(\"./cifar_data/\", train=False, download=True, transform=transform_test)\n",
        "train_val_dataset = CIFAR10(\"./cifar_data/\", train=True, download=True, transform=transform_augment)\n",
        "\n",
        "train_dataset, val_dataset = train_val_split(train_val_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, opt, train_dataset, val_dataset, num_epochs:int = 100, batch_size:int = 64, stop:int = 7, device:str = None):\n",
        "    \"\"\"\n",
        "    A function for training torch models that I kindly took from the professor (sorry) but now with torch dataloaders\n",
        "\n",
        "    Args:\n",
        "        model: torch model to train\n",
        "        opt: optimizer for that model\n",
        "        X_train: training data [n, m], where n is the number of entries and m --- the number of features\n",
        "        y_train: training targets [n,]\n",
        "        X_val: validation data\n",
        "        y_val: validation targets\n",
        "        num_epochs: total amount of full passes over training data (default: 100)\n",
        "        batch_size: number of samples processed in one SGD iteration (default: 64)\n",
        "        stop: number of iterations that loss can decrease for before the training proces stops (default: 7)\n",
        "    \"\"\"\n",
        "    train_loss = []\n",
        "    val_accuracy = []\n",
        "    best_val_acc = 0.0\n",
        "    best_epoch = 0\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "    val_dataloader = torch.utils.data.DataLoader(\n",
        "        val_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # In each epoch, we do a full pass over the training data:\n",
        "        start_time = time.time()\n",
        "        model.train(True) # enable dropout / batch_norm training behavior\n",
        "        for (X_batch, y_batch) in train_dataloader:\n",
        "            X_batch = X_batch.to(torch.float32).to(device)\n",
        "            y_batch = y_batch.to(torch.int64).to(device)\n",
        "            # train on batch\n",
        "            logits = model(X_batch)\n",
        "            loss = F.cross_entropy(logits, y_batch).mean()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            train_loss.append(loss.item())  # .item() = convert 1-value Tensor to float\n",
        "\n",
        "        # And a full pass over the validation data:\n",
        "        model.train(False)     # disable dropout / use averages for batch_norm\n",
        "        with torch.no_grad():  # do not store intermediate activations\n",
        "            for (X_batch, y_batch) in val_dataloader:\n",
        "                X_batch = X_batch.to(torch.float32).to(device)\n",
        "                y_batch = y_batch.detach().cpu().numpy()\n",
        "                logits = model(X_batch)\n",
        "                y_pred = logits.argmax(-1).detach().cpu().numpy()\n",
        "                val_accuracy.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "        mean_val_acc = np.mean(val_accuracy[-len(val_dataset) // batch_size :])\n",
        "\n",
        "        if best_val_acc < mean_val_acc:\n",
        "            best_val_acc = mean_val_acc\n",
        "            best_epoch = i\n",
        "            pathlib.Path(\"./models/\").mkdir(exist_ok=True)\n",
        "            torch.save(model.state_dict(), f\"models/best_model.pt2\")\n",
        "\n",
        "\n",
        "        if epoch - best_epoch > stop:\n",
        "            print(\"Model did not see any loss improvements for %i epochs, aborting...\" % (stop))\n",
        "            model.load_state_dict(torch.load(f\"models/best_model.pt2\", weights_only=True))\n",
        "            model.eval()\n",
        "            break\n",
        "\n",
        "        # Then we print the results for this epoch:\n",
        "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "            epoch + 1, num_epochs, time.time() - start_time))\n",
        "        print(\"  training loss (in-iteration): \\t{:.6f}\".format(\n",
        "            np.mean(train_loss[-len(train_dataset) // batch_size :])))\n",
        "        print(\"  validation accuracy: \\t\\t\\t{:.2f} %\".format(\n",
        "            mean_val_acc * 100))\n",
        "\n",
        "    print(f\"Finished training. Best validation accuracy: {best_val_acc*100:.2f} %\")"
      ],
      "metadata": {
        "id": "BhUK5AIV0oNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, test_dataset):\n",
        "    \"\"\"\n",
        "    A function for evaluating torch models but now with torch dataloaders\n",
        "\n",
        "    Args:\n",
        "        model: torch model to evaluate\n",
        "        test_dataset: torch.Dataset that contains test data\n",
        "    \"\"\"\n",
        "    model.train(False) # disable dropout / use averages for batch_norm\n",
        "    test_dataloader = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=500, shuffle=True, num_workers=1)\n",
        "    test_batch_acc = []\n",
        "\n",
        "    for X_batch, y_batch in test_dataloader:\n",
        "        X_batch = X_batch.to(torch.float32).to(device)\n",
        "        y_batch = y_batch.detach().cpu().numpy()\n",
        "        logits = model(X_batch)\n",
        "        y_pred = logits.max(1)[1].data.cpu().numpy()\n",
        "        test_batch_acc.append(np.mean(y_batch == y_pred))\n",
        "\n",
        "    test_accuracy = np.mean(test_batch_acc)\n",
        "\n",
        "    print(\"Final results:\")\n",
        "    print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "        test_accuracy * 100))"
      ],
      "metadata": {
        "id": "EhqMdrWG_8gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Conv2d(3, 10, kernel_size=(3,3)),  # [10, 30, 30]\n",
        "    nn.GELU(),\n",
        "    nn.MaxPool2d(2), # [10, 15, 15]\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(10 * 15 * 15, 100),\n",
        "    nn.GELU(),\n",
        "    nn.Dropout(p=0.1),\n",
        "    nn.Linear(100, 10)\n",
        ").to(device)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(model, opt, train_sets[\"train\"], train_sets[\"val\"], device=device, stop=10)\n",
        "evaluate(model, test_dataset)"
      ],
      "metadata": {
        "id": "Z3FnyxjarXcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "name": "homework.ipynb",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}